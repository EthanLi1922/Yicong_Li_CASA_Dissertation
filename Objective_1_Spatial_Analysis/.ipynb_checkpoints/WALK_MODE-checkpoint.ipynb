{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:600px\">Objective 1: Spatial Analysis: Accessibility</h1>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last executed: 2024-08-29 02:28:46\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiona & Gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.5\n",
      "3.6.4\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "from osgeo import gdal\n",
    "\n",
    "print(fiona.__version__)\n",
    "print(gdal.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.2\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "print(gpd.__version__)\n",
    "# import r5py as r5py\n",
    "# from r5py.sampledata import helsinki, sao_paulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyproj Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:27700\n"
     ]
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "crs = pyproj.CRS(\"EPSG:27700\")\n",
    "print(crs)\n",
    "# EPSG:27700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# import seaborn as sn\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.1\n"
     ]
    }
   ],
   "source": [
    "import r5py\n",
    "print(r5py.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: r5py\n",
      "Version: 0.1.1\n",
      "Summary: Python wrapper for the R5 routing analysis engine\n",
      "Home-page: \n",
      "Author: Willem Klumpenhouwer, Marcus Sairava, Rafael Pereira, Henrikki Tenkanen\n",
      "Author-email: Christoph Fink <christoph.fink@helsinki.fi>\n",
      "License: GPL-3.0-or-later or MIT\n",
      "Location: C:\\Users\\avb19\\anaconda3\\envs\\r5py\\Lib\\site-packages\n",
      "Requires: ConfigArgParse, filelock, fiona, geopandas, importlib-resources, joblib, jpype1, numpy, pandas, psutil, pyproj, requests, shapely\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show r5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other_Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapely version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "import shapely\n",
    "print(\"shapely version:\", shapely.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transport network Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtually all operations of r5py require a transport network. R5py understands and reads the following types of transport networks:\n",
    "\n",
    "a street network, including infrastructure for cycling and walking, is loaded from an OpenStreetMap extract in Protocol Buffer (.pbf) format (mandatory)\n",
    "\n",
    "a public transport schedule from one or more GTFS files (optional).\n",
    "\n",
    "For the quickstart example, you find sample data sets in the r5py.sampledata.helsinki package (see above).\n",
    "\n",
    "To import the street and public transport networks, instantiate an r5py.TransportNetwork with the file paths to the OSM extract and to zero or more GTFS files. With the sample data set, the file paths are in the r5py.sampledata.helsinki namespace:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the analysis of Greater London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 London LSOAs Geojson File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "Data/London_second_phase/London_LSOA_2021.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: Data/London_second_phase/London_LSOA_2021.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reading geojson files \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m London_LSOA_gdf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/London_second_phase/London_LSOA_2021.geojson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\r5py\\Lib\\site-packages\\geopandas\\io\\file.py:297\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\r5py\\Lib\\site-packages\\geopandas\\io\\file.py:338\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[0;32m    339\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\r5py\\Lib\\site-packages\\fiona\\env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\r5py\\Lib\\site-packages\\fiona\\__init__.py:292\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    303\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    304\u001b[0m         path,\n\u001b[0;32m    305\u001b[0m         mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\r5py\\Lib\\site-packages\\fiona\\collection.py:243\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:588\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:143\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: Data/London_second_phase/London_LSOA_2021.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Import Libs\n",
    "import geopandas as gpd\n",
    "\n",
    "# Reading geojson files \n",
    "London_LSOA_gdf = gpd.read_file(\"Data/London_second_phase/London_LSOA_2021.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London_LSOA_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GeoDataFrame to DataFrame\n",
    "London_LSOA_dropped = London_LSOA_gdf.to_crs(epsg='27700')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 London Library locations: Cultural Infrastructure Map 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CIM_library = pd.read_csv('Data/London/CIM 2023 Libraries (Nov 2023).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CIM_library.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame of a geographic coordinate system\n",
    "\n",
    "# EPSG=4326\n",
    "gdf_geo = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "gdf_geo = gdf_geo.set_crs(epsg=4326)\n",
    "\n",
    "# EPSG=27700\n",
    "gdf_proj = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.easting, df.northing))\n",
    "gdf_proj = gdf_proj.set_crs(epsg=27700)\n",
    "\n",
    "# Verify\n",
    "print(gdf_geo.crs)\n",
    "print(gdf_proj.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_proj.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check files loading condition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import geopandas as gpd\n",
    "\n",
    "# # Set the figure size\n",
    "# fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "# # Plot the London LSOA GeoDataFrame\n",
    "# London_LSOA_dropped.plot(ax=ax, color='white', edgecolor='black')\n",
    "\n",
    "# # Plot the library GeoDataFrame, using red color\n",
    "# gdf_proj.plot(ax=ax, color='red', markersize=30, label='Libraries')\n",
    "\n",
    "# # Add a legend\n",
    "# plt.legend()\n",
    "\n",
    "# # Add a title\n",
    "# plt.title(\"London LSOA and Libraries\")\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 London OSM PBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrosm import OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check files loading condition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path\n",
    "# file_path = \"Data/London/greater_london_latest_0702.osm.pbf\"\n",
    "\n",
    "# # Read the OSM PBF file\n",
    "# osm = OSM(file_path)\n",
    "\n",
    "# # Get road network data\n",
    "# nodes, edges = osm.get_network(nodes=True, network_type=\"driving\")\n",
    "\n",
    "# # Print basic information about nodes and edges\n",
    "# print(\"Nodes:\")\n",
    "# # print(nodes.head())\n",
    "# print(\"\\nEdges:\")\n",
    "# # print(edges.head())\n",
    "\n",
    "# # Check the column names of the node data\n",
    "# print(\"Node columns:\", nodes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a NetworkX graph\n",
    "# G = nx.MultiDiGraph()\n",
    "\n",
    "# # Add nodes\n",
    "# for _, node in nodes.iterrows():\n",
    "#     G.add_node(node[\"id\"], lat=node[\"lat\"], lon=node[\"lon\"])\n",
    "\n",
    "# # Add edges\n",
    "# for _, edge in edges.iterrows():\n",
    "#     G.add_edge(edge[\"u\"], edge[\"v\"], id=edge[\"id\"])\n",
    "\n",
    "# # Draw the graph\n",
    "# fig, ax = plt.subplots(figsize=(30, 30))\n",
    "# pos = {node: (data['lon'], data['lat']) for node, data in G.nodes(data=True)}\n",
    "# nx.draw(G, pos, node_size=1, node_color='red', edge_color='black', linewidths=0.3, width=0.5, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 London GTFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources: https://data.bus-data.dft.gov.uk/timetable/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "zip_file_path = 'Data/London_second_phase/itm_london_gtfs.zip' \n",
    "\n",
    "# List the contents of the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_file_list = zip_ref.namelist()\n",
    "\n",
    "print(\"ZIP file contents:\", zip_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "zip_file_path = 'Data/London_second_phase/itm_london_gtfs.zip'\n",
    "extract_path = 'Data/London_second_phase/itm_london_gtfs'\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(extract_path)\n",
    "print(\"Extracted files:\", extracted_files)\n",
    "\n",
    "# Read the main GTFS files\n",
    "def read_gtfs_file(filename):\n",
    "    file_path = os.path.join(extract_path, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "    else:\n",
    "        print(f\"{filename} does not exist.\")\n",
    "        return None\n",
    "\n",
    "agency = read_gtfs_file('agency.txt')\n",
    "stops = read_gtfs_file('stops.txt')\n",
    "routes = read_gtfs_file('routes.txt')\n",
    "trips = read_gtfs_file('trips.txt')\n",
    "stop_times = read_gtfs_file('stop_times.txt')\n",
    "calendar = read_gtfs_file('calendar.txt')\n",
    "calendar_dates = read_gtfs_file('calendar_dates.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. London Transport network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create a R5py \"TransportNetwork\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import r5py\n",
    "\n",
    "# Use local file paths\n",
    "osm_pbf_path = 'Data/London/greater_london_latest_0702.osm.pbf'\n",
    "gtfs_path = 'Data/London_second_phase/itm_london_gtfs.zip'\n",
    "# gtfs_path = 'Data/London/london-dft.zip'\n",
    "\n",
    "# Create the TransportNetwork object\n",
    "transport_network = r5py.TransportNetwork(\n",
    "    osm_pbf_path,\n",
    "    [gtfs_path]\n",
    ")\n",
    "\n",
    "print(\"TransportNetwork created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the object\n",
    "print(\"Type of transport_network:\", type(transport_network))\n",
    "\n",
    "# Check the attributes and methods of the object\n",
    "print(\"Attributes and methods of transport_network:\", dir(transport_network))\n",
    "\n",
    "# If you want to view detailed information about a specific attribute or method, you can use help()\n",
    "# For example, to check details about the path analysis method\n",
    "if hasattr(transport_network, 'path_analysis'):\n",
    "    print(\"Help on path_analysis method:\")\n",
    "    print(help(transport_network.path_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute a travel time matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Origins: LSOAs centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = London_LSOA_dropped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins['id'] = origins['FID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origins.crs\n",
    "# #<Projected CRS: EPSG:27700>; Name: OSGB36 / British National Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins.head(2)\n",
    "# 这时geometry列为\"polygon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_1 = origins.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_1.geometry = origins_1.geometry.centroid\n",
    "# 将geometry列从\"polygon\"转换为多边形的中心点\"point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot to check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "# Plot the GeoDataFrame\n",
    "London_LSOA_dropped.plot(ax=ax, color='white', edgecolor='black')\n",
    "origins.plot(ax=ax, color='red', edgecolor='black')\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"London LSOA Geometry Map\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Destinations: Public library locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_all = gdf_proj.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_all[\"id\"] = destinations_all.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_1 = destinations_all.iloc[230:231]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 travel_time_matrix_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember: appropriate datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_time_matrix_computer_London = r5py.TravelTimeMatrixComputer(\n",
    "    transport_network,\n",
    "    origins=origins_1,\n",
    "    destinations=destinations_1,\n",
    "    departure=datetime.datetime(2024, 7, 23, 14, 0),\n",
    "    transport_modes=[\n",
    "        # r5py.TransportMode.TRANSIT,\n",
    "        r5py.TransportMode.WALK,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_time_matrix_computer_London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_London_1 = travel_time_matrix_computer_London.compute_travel_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_London_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_walk = origins_1.merge(travel_times_London_1, left_on=\"id\", right_on=\"from_id\")\n",
    "travel_times_walk.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot only one public library as the destination to check the function works successfully —— In points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # This replaces LSOA with the central point\n",
    "\n",
    "# travel_times_map_walk = travel_times_walk.explore(\"travel_time\", cmap=\"Blues\")\n",
    "\n",
    "# # # This line of code adds the locations of train stations to the previously created population distribution map, using markers to represent the train stations.\n",
    "# # travel_times_map_walk = destinations_1.explore(m=travel_times_map_walk, marker_type=\"marker\")\n",
    "\n",
    "# travel_times_map_walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot only one public library as the destination to check the function works successfully —— In areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_area_1 = London_LSOA_dropped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_area_1['id'] = origins_area_1['FID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_walk_1 = origins_area_1.merge(travel_times_London_1, left_on=\"id\", right_on=\"from_id\")\n",
    "travel_times_walk_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_map_walk_1 = travel_times_walk_1.explore(\"travel_time\", cmap=\"Blues\")\n",
    "\n",
    "# # This line of code adds the locations of train stations to the previously created population distribution map, using markers to represent the train stations.\n",
    "# travel_times_map_walk_1 = destinations_1.explore(m=travel_times_map_walk_1, marker_type=\"marker\")\n",
    "\n",
    "travel_times_map_walk_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save a HTML to further analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path = \"Plot_final/WALK_id230_0822.html\"\n",
    "travel_times_map_walk_1.save(html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute the whole OD pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Load the python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(shapely.__version__)\n",
    "# print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Destinations: Public library locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries_gdf contains information about libraries and the geometry column\n",
    "# Libraries are the destination\n",
    "libraries_gdf_1 = destinations_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries_gdf_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries_gdf_1.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries_gdf_1['Library_Index'] = libraries_gdf_1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries_gdf_1.crs\n",
    "# # <Projected CRS: EPSG:27700>\n",
    "# # Name: OSGB36 / British National Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Origins: LSOAs centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsoa_gdf contains LSOA IDs and the coordinates of the centroid\n",
    "lsoa_gdf_1 = origins_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R5py requires a column named \"id\" as the default input —— resulting in \"from_id\" and \"to_id\"\n",
    "lsoa_gdf_1['id'] = lsoa_gdf_1['FID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsoa_gdf_1.crs\n",
    "# # <Projected CRS: EPSG:27700>\n",
    "# # Name: OSGB36 / British National Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Nearest Neighbor Analysis: To make up OD pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Ensure all data uses the same Coordinate Reference System (CRS)\n",
    "print(\"LSOA CRS:\", lsoa_gdf_1.crs)\n",
    "print(\"Libraries CRS:\", libraries_gdf_1.crs)\n",
    "\n",
    "# Ensure the coordinate reference system is EPSG:27700\n",
    "assert lsoa_gdf_1.crs == libraries_gdf_1.crs == \"EPSG:27700\"\n",
    "\n",
    "# Extract the coordinates of the LSOA centroids\n",
    "lsoa_coords = [(geom.x, geom.y) for geom in lsoa_gdf_1.geometry]\n",
    "print(\"LSOA Coordinates:\", lsoa_coords[:5])   # Print the first 5 LSOA coordinates for verification\n",
    "\n",
    "# Extract the coordinates of the libraries\n",
    "library_coords = [(geom.x, geom.y) for geom in libraries_gdf_1.geometry]\n",
    "print(\"Library Coordinates:\", library_coords[:5])  # Print the first 5 library coordinates for verification\n",
    "\n",
    "\n",
    "\n",
    "# Perform nearest neighbor analysis using cKDTree\n",
    "tree = cKDTree(library_coords)\n",
    "distances, indices = tree.query(lsoa_coords, k=1)\n",
    "\n",
    "print(\"Distances:\", distances[:5])  # Print the first 5 distances for verification\n",
    "print(\"Indices:\", indices[:5])      # Print the first 5 indices for verification\n",
    "\n",
    "# Check the indices and data type of the library DataFrame\n",
    "print(\"Library DataFrame Index:\", libraries_gdf_1.index)\n",
    "\n",
    "# Add the results to the LSOA GeoDataFrame\n",
    "lsoa_gdf_1['Nearest_Library_ID'] = libraries_gdf_1.iloc[indices].index.values\n",
    "lsoa_gdf_1['Distance_to_Library'] = distances\n",
    "\n",
    "# Output the results\n",
    "print(lsoa_gdf_1[['LSOA21CD', 'Nearest_Library_ID', 'Distance_to_Library']].head())\n",
    "\n",
    "\n",
    "\n",
    "# Create a new GeoDataFrame to store the lines from LSOA centroids to the nearest libraries\n",
    "lines = []\n",
    "\n",
    "for i, row in lsoa_gdf_1.iterrows():\n",
    "    lsoa_point = row.geometry\n",
    "    library_index = row['Nearest_Library_ID']\n",
    "    library_point = libraries_gdf_1.loc[library_index].geometry\n",
    "    line = LineString([lsoa_point, library_point])\n",
    "    lines.append(line)\n",
    "\n",
    "lines_gdf = gpd.GeoDataFrame(geometry=lines, crs=\"EPSG:27700\")\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# Plot LSOA centroids\n",
    "lsoa_gdf_1.plot(ax=ax, color='blue', markersize=3, label='LSOA Centers')\n",
    "# Plot library locations\n",
    "libraries_gdf_1.plot(ax=ax, color='red', markersize=10, label='Libraries')\n",
    "# Plot the lines\n",
    "lines_gdf.plot(ax=ax, color='black', linewidth=0.8, linestyle='--', label='Nearest Path')\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "# Add a title\n",
    "plt.title('Nearest Library for Each LSOA Center')\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Calculate Whole OD pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_2 = lsoa_gdf_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assuming lsoa_gdf_1 is a GeoDataFrame that already contains the Nearest_Library_ID column\n",
    "# Count the number of unique values in the Nearest_Library_ID column\n",
    "unique_library_ids = lsoa_gdf_2['Nearest_Library_ID'].nunique()\n",
    "\n",
    "unique_library_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_0 = lsoa_gdf_2[lsoa_gdf_2['Nearest_Library_ID'] == 0]\n",
    "ID_0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_2['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_2['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 LSOA dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create a dictionary: lsoa_origin_{idx} The idx means LSOAs \"Nearest_Library_ID\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Assuming libraries_gdf_2 is a GeoDataFrame that already contains the Library_Index column\n",
    "unique_indices_lsoa = lsoa_gdf_2['Nearest_Library_ID'].unique()\n",
    "\n",
    "# Create a dictionary to store DataFrames for different index values\n",
    "lsoa_origin_dict = {}\n",
    "\n",
    "# Iterate over each unique index value\n",
    "for idx in unique_indices_lsoa:\n",
    "    # Filter lsoa_gdf_2 based on the index value\n",
    "    lsoa_origin_dict[f'lsoa_origin_{idx}'] = lsoa_gdf_2[lsoa_gdf_2['Nearest_Library_ID'] == idx]\n",
    "\n",
    "# # Check the created DataFrames\n",
    "# for key, df in lsoa_origin_dict.items():\n",
    "#     print(f\"{key}:\\n{df}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps some libraries are not associated with any LSOA\n",
    "unique_indices_lsoa.size\n",
    "\n",
    "# Result: All public libraries serve as the destination for at least one LSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \"Nearest_Library_ID\" is 340\n",
    "lsoa_origin_dict[f'lsoa_origin_{340}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each DataFrame in the dictionary\n",
    "for key, df in lsoa_origin_dict.items():\n",
    "    # Check if there are any NA values in the 'Nearest_Library_ID' column\n",
    "    na_exists = df['Nearest_Library_ID'].isna().any()\n",
    "    \n",
    "    # Find the maximum and minimum values in the 'Distance_to_Library' column\n",
    "    max_distance = df['Distance_to_Library'].max()\n",
    "    min_distance = df['Distance_to_Library'].min()\n",
    "    \n",
    "    # Add the results to the list\n",
    "    results.append({\n",
    "        'Dataset': key,\n",
    "        'Nearest_Library_ID_has_NA': na_exists,\n",
    "        'Max_Distance_to_Library': max_distance,\n",
    "        'Min_Distance_to_Library': min_distance\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_null = results_df[results_df[\"Nearest_Library_ID_has_NA\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Max_Distance_to_Library\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Min_Distance_to_Library\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Public libraries dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create a new column \"id\" to run R5py function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries_gdf_2 = libraries_gdf_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create the Library_Index column\n",
    "libraries_gdf_2 = libraries_gdf_2.reset_index(drop=True)\n",
    "libraries_gdf_2['Library_Index'] = libraries_gdf_2.index\n",
    "\n",
    "libraries_gdf_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Assuming libraries_gdf_2 is a GeoDataFrame that already contains the Library_Index column\n",
    "unique_indices_libraries = libraries_gdf_2['Library_Index'].unique()\n",
    "\n",
    "# Create a dictionary to store DataFrames for different index values\n",
    "libraries_destination_dict = {}\n",
    "\n",
    "# Iterate over each unique index value\n",
    "for idx in unique_indices_libraries:\n",
    "    # Filter libraries_gdf_2 based on the index value\n",
    "    libraries_destination_dict[f'libraries_destination_{idx}'] = libraries_gdf_2[libraries_gdf_2['Library_Index'] == idx]\n",
    "\n",
    "# # Check the created DataFrames\n",
    "# for key, df in libraries_destination_dict.items():\n",
    "#     print(f\"{key}:\\n{df}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_indices_libraries.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries_destination_dict['libraries_destination_346']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Compute R5py \"TravelTimeMatrixComputer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_time_matrix_computer_London_final_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import r5py\n",
    "import datetime\n",
    "\n",
    "# libraries_destination_dict = {...}\n",
    "# lsoa_origin_dict = {...}\n",
    "# transport_network = ...\n",
    "\n",
    "# Create a dictionary to store TravelTimeMatrixComputer objects\n",
    "travel_time_matrix_computer_London_final_dict = {}\n",
    "\n",
    "# Iterate over each unique index value, create TravelTimeMatrixComputer objects, and store them in the dictionary\n",
    "for idx in unique_indices_lsoa:\n",
    "    key = f'travel_time_matrix_computer_London_final_{idx}'\n",
    "    travel_time_matrix_computer_London_final_dict[key] = r5py.TravelTimeMatrixComputer(\n",
    "        transport_network,\n",
    "        origins=lsoa_origin_dict[f'lsoa_origin_{idx}'],\n",
    "        destinations=libraries_destination_dict[f'libraries_destination_{idx}'],\n",
    "        departure=datetime.datetime(2024, 7, 23, 14, 0),\n",
    "        transport_modes=[\n",
    "            # r5py.TransportMode.TRANSIT,\n",
    "            r5py.TransportMode.WALK,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# # Check the created dictionary\n",
    "# for key, value in travel_time_matrix_computer_London_final_dict.items():\n",
    "#     print(f\"{key}:\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_time_matrix_computer_London_final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dictionary, assigning each value to a global variable named after the key\n",
    "for key, value in travel_time_matrix_computer_London_final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# Check some of the newly created global variables\n",
    "print(travel_time_matrix_computer_London_final_340)\n",
    "print(travel_time_matrix_computer_London_final_149)\n",
    "print(travel_time_matrix_computer_London_final_151)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_time_matrix_computer_London_final_340.compute_travel_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_travel_times_and_store_results():\n",
    "    # Iterate through each key in the dictionary\n",
    "    for key in travel_time_matrix_computer_London_final_dict.keys():\n",
    "        # Dynamically get the original variable\n",
    "        computer = globals()[key]\n",
    "        \n",
    "        # Compute travel times\n",
    "        travel_times = computer.compute_travel_times()\n",
    "        \n",
    "        # Dynamically create a new variable name\n",
    "        new_var_name = key.replace('travel_time_matrix_computer_London_final_', 'travel_times_London_final_')\n",
    "        \n",
    "        # Store the result in the new variable\n",
    "        globals()[new_var_name] = travel_times\n",
    "\n",
    "# Call the function to perform the computation and store the results\n",
    "compute_travel_times_and_store_results()\n",
    "\n",
    "# Check the result\n",
    "print(travel_times_London_final_151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the values of all variables\n",
    "all_travel_times = []\n",
    "\n",
    "# First, collect all variable names that meet the condition\n",
    "variable_names = [var_name for var_name in globals() if var_name.startswith(\"travel_times_London_final_\")]\n",
    "\n",
    "# Iterate over the collected variable names\n",
    "for var_name in variable_names:\n",
    "    # Get the value of the variable and add it to the list\n",
    "    all_travel_times.append(globals()[var_name])\n",
    "\n",
    "# Concatenate all results into a single DataFrame or GeoDataFrame\n",
    "travel_times_London_final_all = pd.concat(all_travel_times, ignore_index=True)\n",
    "\n",
    "# Check the merged results\n",
    "travel_times_London_final_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Ex1 Check: If \"travel_time\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows where the travel_time column has null values\n",
    "null_travel_times = travel_times_London_final_all[travel_times_London_final_all[\"travel_time\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_travel_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_London_final_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_London_final_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_gdf_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_walk_final_all = lsoa_gdf_2.merge(travel_times_London_final_all, left_on=\"id\", right_on=\"from_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Ex2 Try to estimate NA —— fail and not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_walk_final_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**https://r5py.readthedocs.io/en/stable/reference/reference.html#r5py.TravelTimeMatrixComputer**\n",
    "\n",
    "**speed_walking (float) – Mean walking speed for routing, km/h. Default: 3.6 km/h ==== 60m/min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_area_all = origins_area_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_area_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times_walk_final_all = origins_area_all.merge(travel_times_London_final_all, left_on=\"id\", right_on=\"from_id\")\n",
    "travel_times_walk_final_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Assuming travel_times_walk_final_all is a GeoDataFrame that contains the travel_time column\n",
    "\n",
    "# Visualize using the explore method, adjusting the color map and boundary color\n",
    "# travel_times_map_walk_final_all = travel_times_walk_final_all.explore(\"travel_time\", cmap=\"Blues\")\n",
    "travel_times_map_walk_final_all = travel_times_walk_final_all.explore(\n",
    "    column=\"travel_time\",\n",
    "    cmap=\"Blues\",\n",
    "    scheme=\"Quantiles\",  # Use the quantiles method for color classification\n",
    "    k=10,  # Divide the color map into 10 levels\n",
    "    edgecolor=\"black\",  # Set the boundary color to black\n",
    "    linewidth=0.5,  # Set the boundary line width\n",
    "    legend=True  # Display the legend\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "travel_times_map_walk_final_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path = \"Plot_final/Plot_WALK_0822.html\"\n",
    "travel_times_map_walk_final_all.save(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pandas has already been imported\n",
    "import pandas as pd\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "travel_times_walk_final_all.to_csv('Result_final/travel_times_WALK_0822.csv', index=False)\n",
    "\n",
    "print(\"The file has been successfully exported.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (r5py)",
   "language": "python",
   "name": "r5py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
